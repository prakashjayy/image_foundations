{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0c55b3e",
   "metadata": {},
   "source": [
    "## [ViTDet](https://arxiv.org/pdf/2203.16527.pdf) - The go to architecture for image foundation models \n",
    "\n",
    "ViTDet, as of Jan 2024 is the go to architecture for all the vision tasks. It is used in `segment-anything`. The [`ViTAE-Transformer`](https://github.com/ViTAE-Transformer) has SOTA on several tasks like semantic segmentation, object detection, human pose, matting, Remote sensing etc. Understanding this backbone architecture will help us in choosing optimal parameters based on the task. \n",
    "\n",
    "Original ViTDet was written to highlight the need for specialized architecture for object detection using transformers. In a way, I will call this a super-simplified `Swin Transformers` which basically removed the heirarical nature of the network, shifted windows etc.\n",
    "\n",
    "Note: we will only talk about the backbone and leave the FPN based ablation studies to the reader. \n",
    "\n",
    "So the network is broadly divided as \n",
    "> [PatchEmbed] -> nx[blocks] -> [Neck]\n",
    "\n",
    "Inside each block, we have \n",
    "- window attention \n",
    "- relative postional encoding. \n",
    "\n",
    "we will talk about all of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8ccdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import fastcore.all as fc\n",
    "from PIL import Image\n",
    "from functools import partial\n",
    "from torchvision.transforms import RandomResizedCrop, RandomHorizontalFlip, Compose, ToTensor, ToPILImage\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"bmh\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269f52d9",
   "metadata": {},
   "source": [
    "> Lets create an image of size 224x224 with a patch size of 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0302c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 1024\n",
    "patch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69961b35",
   "metadata": {},
   "source": [
    "## load and visualize an image\n",
    "\n",
    "we load and use `coco val` data. For this blog purpose, u can pick up any image of your choice from the internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49734a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#5000) [Path('coco/val2017/000000182611.jpg'),Path('coco/val2017/000000335177.jpg'),Path('coco/val2017/000000278705.jpg'),Path('coco/val2017/000000463618.jpg'),Path('coco/val2017/000000568981.jpg'),Path('coco/val2017/000000092416.jpg'),Path('coco/val2017/000000173830.jpg'),Path('coco/val2017/000000476215.jpg'),Path('coco/val2017/000000479126.jpg'),Path('coco/val2017/000000570664.jpg')...]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs = fc.L(fc.Path(\"coco/val2017/\").glob(\"*.jpg\"))\n",
    "imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb71cbcc",
   "metadata": {},
   "source": [
    "> The following are the standard transforms mentioned in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333d8d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms():\n",
    "    return Compose([RandomResizedCrop(size=1024, scale=[0.4, 1], ratio=[0.75, 1.33], interpolation=2), \n",
    "                    RandomHorizontalFlip(p=0.5), \n",
    "                    ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8936302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(img_loc, transforms):\n",
    "    img = Image.open(img_loc)\n",
    "    return transforms(img)\n",
    "\n",
    "load_img = partial(load_img, transforms=transforms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfddd12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1024, 1024])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = load_img(imgs[1])\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201637ef",
   "metadata": {},
   "source": [
    "## Patch Embed\n",
    "\n",
    "we will create patch embeddings for [3x32x32]. For this we can use a simple convolution layer with kernel and stride as patch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1251155f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_channels = 3\n",
    "hidden_size = 768\n",
    "projection = nn.Conv2d(num_channels, hidden_size, kernel_size=patch_size, stride=patch_size)\n",
    "projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f171d037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768, 32, 32])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe = projection(img.unsqueeze(0))\n",
    "pe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00771768",
   "metadata": {},
   "source": [
    "> reshuffle the pixels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9019cd99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 32, 768])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe = pe.permute((0, 2, 3, 1))\n",
    "pe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dcf9aa",
   "metadata": {},
   "source": [
    "> Now we have [32x32] = 1024 tokens with each token of 768 vectors. The positions of each token wrt to other is preserved using conv type structure.\n",
    "\n",
    "> we can add positial encodings to these features as optional. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bc75cd",
   "metadata": {},
   "source": [
    "## Transformer Blocks \n",
    "\n",
    "In each transformer block, we first apply windowing, Then calculate attention, reattach window blocks, apply mlp. The transformer block also has a few skip connection and normalization layers as shown below.\n",
    "\n",
    "<img src=\"images/vitdet_block.png\" width=250 height=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3ae04e",
   "metadata": {},
   "source": [
    "## Windowing\n",
    "In the context of `ViTDet` windowing is optional and attention can be calculated on all the tokens. This type of attention is called `global attention`. But global attention is expensive as we have to calculate a matrix of 1024x1024 in this case. If the patch_size is much smaller this will quadaritcally increase in size making it very expensive to compute. So window attention is considered, \n",
    "\n",
    "- First the 32x32 matrix is divided into 8x8 (window_size) windows. So we will get a total of (32/8) * (32/8) = 16 windows, with each window having (8x8) 64 tokens. Attention is only calculated within these tokens making it a `local attention`.\n",
    "\n",
    "<img src=\"images/vitdet_windows.png\" width=400 height=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861ecfc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 8, 4, 8, 768])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_size = 8\n",
    "batch_size, height, width, num_channels = pe.shape\n",
    "wpe = pe.view(\n",
    "        batch_size, height // window_size, window_size, width // window_size, window_size, num_channels\n",
    "    )\n",
    "wpe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7687eb20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 8, 8, 768])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windows = wpe.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, num_channels)\n",
    "windows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a311bbfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 64, 768])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windows = windows.view(-1, window_size*window_size, num_channels)\n",
    "windows.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a667fc",
   "metadata": {},
   "source": [
    "## Attention\n",
    "This is a simple attention as discussed in [`attention is all you need`](https://arxiv.org/pdf/1706.03762.pdf) paper. we will see step by step as follows \n",
    "\n",
    "<img src=\"images/vitdet_attention.png\" width=200 height=200>\n",
    "\n",
    "> we obtain q, k, v matrices by using MLP layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d924dd12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Linear(in_features=768, out_features=192, bias=True),\n",
       "  Linear(in_features=768, out_features=192, bias=True),\n",
       "  Linear(in_features=768, out_features=192, bias=True),\n",
       "  Linear(in_features=768, out_features=192, bias=True)],\n",
       " [Linear(in_features=768, out_features=192, bias=True),\n",
       "  Linear(in_features=768, out_features=192, bias=True),\n",
       "  Linear(in_features=768, out_features=192, bias=True),\n",
       "  Linear(in_features=768, out_features=192, bias=True)],\n",
       " [Linear(in_features=768, out_features=192, bias=True),\n",
       "  Linear(in_features=768, out_features=192, bias=True),\n",
       "  Linear(in_features=768, out_features=192, bias=True),\n",
       "  Linear(in_features=768, out_features=192, bias=True)])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = windows.shape[-1]\n",
    "num_heads = 4\n",
    "head_dim = dim // num_heads\n",
    "scale = head_dim**-0.5\n",
    "wq = [nn.Linear(dim, head_dim) for head in range(num_heads)]\n",
    "wk = [nn.Linear(dim, head_dim) for head in range(num_heads)]\n",
    "wv = [nn.Linear(dim, head_dim) for head in range(num_heads)]\n",
    "wq, wk, wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6150b274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([16, 64, 192]),\n",
       " torch.Size([16, 64, 192]),\n",
       " torch.Size([16, 64, 192]),\n",
       " torch.Size([16, 64, 192])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = [i(windows) for i in wq]\n",
    "k = [i(windows) for i in wk]\n",
    "v = [i(windows) for i in wv]\n",
    "[i.shape for i in q] ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d30562a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 64, 192]),\n",
       " torch.Size([64, 64, 192]),\n",
       " torch.Size([64, 64, 192]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = torch.concatenate(q) # number of heads * windows\n",
    "k = torch.concatenate(k)\n",
    "v = torch.concatenate(v)\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912125fb",
   "metadata": {},
   "source": [
    "> Matmul of q and k and use scale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d0bf7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores = (q @ k.transpose(-2, -1)) * scale\n",
    "attention_scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e778dbe",
   "metadata": {},
   "source": [
    "> Apply relative positional encodings\n",
    "\n",
    "This is a separate topic of its own to discuss but essentially we will add positional encodings in each attention block instead of at the start as done in plain vanilla vit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67d8176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([15, 192]), torch.Size([15, 192]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_pos_h = nn.Parameter(torch.zeros(2 * window_size - 1, head_dim))\n",
    "rel_pos_w = nn.Parameter(torch.zeros(2 * window_size - 1, head_dim))\n",
    "rel_pos_h.shape, rel_pos_w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89f3761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.vitdet.modeling_vitdet import add_decomposed_relative_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d976161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores = add_decomposed_relative_positions(\n",
    "                attention_scores, q, rel_pos_h, rel_pos_w, (window_size, window_size), (window_size, window_size)\n",
    "            )\n",
    "attention_scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04b9de8",
   "metadata": {},
   "source": [
    "> Apply softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997571a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_probs = attention_scores.softmax(dim=-1)\n",
    "attention_probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baecd4f",
   "metadata": {},
   "source": [
    "> Multiply by key vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772cc1ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64, 192])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_state = attention_probs @ v\n",
    "hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571ddb45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 8, 8, 768])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_state = hidden_state.view(16, num_heads, window_size, window_size, -1)\n",
    "hidden_state = hidden_state.permute(0, 2, 3, 1, 4)\n",
    "hidden_state = hidden_state.reshape(16, window_size, window_size, -1)\n",
    "hidden_state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f03ff39",
   "metadata": {},
   "source": [
    "> Add projection layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe35e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=768, bias=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj = nn.Linear(dim, dim)\n",
    "proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee630bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 8, 8, 768])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_out = proj(hidden_state)\n",
    "attention_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef0b3d2",
   "metadata": {},
   "source": [
    "## Unwindowing \n",
    "un window the existing vector and get it in the form of (batch_size, tokens, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1013656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 32, 768])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe = attention_out.view(-1, height // window_size, width // window_size, \\\n",
    "                       window_size, window_size, num_channels)\n",
    "pe = pe.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, height, width, num_channels)\n",
    "pe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c181c86",
   "metadata": {},
   "source": [
    "> we have done windowing - applied attention - unwindowed to get the vector.\n",
    "\n",
    "> U can see that, the output vector size is same as input.\n",
    "\n",
    "> if you don't want to global attention we can set the window_size as input size . in this case it is 32x32."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecbe0c4",
   "metadata": {},
   "source": [
    "## Residual block \n",
    "In the network so far we have seen that attention is applied only within the windows. To learn accross windows, we did apply global attention in some of the layers. Global attention is considered to be expensive and is so applied only in few cases.\n",
    "- The network is divided into 4 subsets. with each subset containing 6 blocks. So there are a total of 24 layers.\n",
    "- At the end of each subset for the final block we apply global attention. \n",
    "\n",
    "This will reduce our computation and also allow tokens to learn outside windows. \n",
    "\n",
    "The authors of the paper also suggested a residual block with conv layer instead of global attention. The network looks as below with 1x1, 3x3 and 1x1 conv layers. this will allow the network to learn from all the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ecc666",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.vitdet.modeling_vitdet import VitDetResBottleneckBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cab54e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VitDetResBottleneckBlock(\n",
       "  (conv1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (norm1): VitDetLayerNorm()\n",
       "  (act1): GELUActivation()\n",
       "  (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (norm2): VitDetLayerNorm()\n",
       "  (act2): GELUActivation()\n",
       "  (conv3): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (norm3): VitDetLayerNorm()\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class config:\n",
    "    hidden_act = \"gelu\"\n",
    "residual = VitDetResBottleneckBlock(config, in_channels=768, out_channels=768, bottleneck_channels=768//2)\n",
    "residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9f0ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768, 32, 32])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residual(pe.permute((0, 3, 1, 2))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbb4cf5",
   "metadata": {},
   "source": [
    "The crux of the network is only this. Now lets define all the parameters in `Segment Anything` backbone and see if everything is making sense. \n",
    "\n",
    "## Full scale network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33c787f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_anything.modeling.image_encoder import ImageEncoderViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af9d521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageEncoderViT(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLPBlock(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (neck): Sequential(\n",
       "    (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (1): LayerNorm2d()\n",
       "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (3): LayerNorm2d()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = ImageEncoderViT(img_size=1024,\n",
    "                      patch_size=16, \n",
    "                      in_chans=3, \n",
    "                      embed_dim=768, \n",
    "                      depth=12, \n",
    "                      num_heads=12, \n",
    "                      mlp_ratio=4, \n",
    "                      out_chans=256, \n",
    "                      qkv_bias=True, \n",
    "                      norm_layer= torch.nn.modules.normalization.LayerNorm, \n",
    "                      act_layer=torch.nn.modules.activation.GELU, \n",
    "                      use_abs_pos=False, \n",
    "                      use_rel_pos=True, \n",
    "                      window_size=16,\n",
    "                      global_attn_indexes=[2, 5, 8, 11])\n",
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530d3632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 64, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc(img.unsqueeze(0)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca7fe70",
   "metadata": {},
   "source": [
    "## Ablation studies. \n",
    "- window attention is sufficient when aided with few global attention blocks. \n",
    "- using residual conv or global attention gave similar performance. Training and inference time is much lower when using residual conv. \n",
    "- Masked Autoencoders provide strong pre-trained backbones\n",
    "- Compared to hierical backbones like MViT2 or Swin Transformers ViTDet works better. \n",
    "- Finally reaches 61.3 APbox on coco test set when pretrained with Imagenet 1k using MAE.\n",
    "\n",
    "In the next series we will understand what is MAE and how we can apply them to plain Vanilla ViTs and ViTDet. Thank you."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medct",
   "language": "python",
   "name": "medct"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
